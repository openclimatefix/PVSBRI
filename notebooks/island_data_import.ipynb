{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8ddecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db63cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- Import Libs ---------\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import math\n",
    "import pytz\n",
    "\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "alt.renderers.enable(\"altair_viewer\")\n",
    "\n",
    "# Disbale the max row limit for altair datasets.\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a8259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory: we use an environment variable defined in the Makefile.\n",
    "CWD = os.environ.get(\"CWD\")\n",
    "if CWD:\n",
    "    os.chdir(CWD)\n",
    "\n",
    "print(CWD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d8c203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function does the same as above but orders the dataframe correct\n",
    "# I should also make the folder path a variable so that I am perform some quick stats analysis on it\n",
    "def folder_data_load_sorted(folder_path):\n",
    "    # This is a list of all of the filenames\n",
    "    files = os.listdir(folder_path)\n",
    "\n",
    "    # Read all Excel files into a list of dataframes\n",
    "    dataframes = []\n",
    "\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".xlsx\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            print(file_path)\n",
    "\n",
    "            # Extract the reference date from the filename\n",
    "            reference_date = filename.split(\"_\")[0]\n",
    "\n",
    "            df = pd.read_excel(file_path, engine=\"openpyxl\")\n",
    "\n",
    "            # create new columns in the data for date specific\n",
    "            df[\"Year\"] = reference_date.split(\"-\")[0]\n",
    "            df[\"Month\"] = reference_date.split(\"-\")[1]\n",
    "            df[\"Day\"] = reference_date.split(\"-\")[2]\n",
    "\n",
    "            # Convert the 'Time' column to datetime.time objects\n",
    "            df[\"Time2\"] = pd.to_datetime(df[\"Time\"], format=\"%H:%M:%S\").dt.time\n",
    "            df[\"Time2\"] = df[\"Time2\"].astype(str)\n",
    "\n",
    "            df[\"Date\"] = pd.to_datetime(df[\"Year\"] + df[\"Month\"] + df[\"Day\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "            # Convert the 'Time' column to datetime objects using the extracted reference date\n",
    "            df[\"Datetime\"] = pd.to_datetime(\n",
    "                reference_date + \" \" + df[\"Time2\"], format=\"%Y-%m-%d %H:%M:%S\"\n",
    "            )\n",
    "\n",
    "            # This returns a tuple\n",
    "            dataframes.append((reference_date, df))\n",
    "\n",
    "    dataframes.sort(key=lambda x: x[0])\n",
    "\n",
    "    sorted_dataframes = [df for _, df in dataframes]\n",
    "\n",
    "    return sorted_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f5aedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best to create a way to look at all of the data now rather then just one specific year.\n",
    "def load_data_from_all_years(parent_folder_path):\n",
    "    # Initialize an empty list to store the dataframes\n",
    "    all_dataframes = []\n",
    "\n",
    "    # Loop over each year's folder and call the folder_data_load_sorted function\n",
    "    for year in range(2018, 2024):\n",
    "        folder_path = os.path.join(parent_folder_path, str(year))\n",
    "        dataframes = folder_data_load_sorted(folder_path)\n",
    "        all_dataframes.extend(dataframes)\n",
    "\n",
    "    return all_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d853af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to load a single excel file\n",
    "def read_excel_file(file_path):\n",
    "    df = pd.read_excel(file_path, engine=\"openpyxl\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcc3fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 15 min data ------------\n",
    "def load_all_15min(parent_folder_path):\n",
    "\n",
    "    all_dataframes = load_data_from_all_years(parent_folder_path)\n",
    "    all_combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "    # Sort the DataFrame based on the 'Datetime' column\n",
    "    all_combined_df_sort = all_combined_df.sort_values(by=\"Datetime\")\n",
    "\n",
    "    return all_combined_df_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c48f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function to transform the dates to fix the bug\n",
    "def transform_date(date_str):\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    date_obj = datetime.strptime(date_str, date_format)\n",
    "    new_date = date_obj.replace(hour=date_obj.minute, minute=date_obj.second, second=0)\n",
    "    return new_date.strftime(date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208e8243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- 15 min data opperation ----------\n",
    "# 1. Load the data into a single list\n",
    "parent_folder_path = \"./data/island_A/clean_data/15-min-PV/\"\n",
    "data_15min = load_all_15min(parent_folder_path)\n",
    "\n",
    "print(\"-------- COMPLETE 1 -------\")\n",
    "\n",
    "# This code here is if you want to just load a single year of 15 min data\n",
    "#     folder_path = \"./data/island_A/15-min-PV/2019/\"\n",
    "#     data_15min = folder_data_load_sorted(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fae72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Fix the bugs with datetime interp\n",
    "# define the bug_start and bug_end dates + May need to enter new dates and check format\n",
    "bug_start = datetime.strptime(\"2018-12-19 23:45:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "bug_end = datetime.strptime(\"2019-10-05 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "data_15min[\"Datetime\"] = pd.to_datetime(data_15min[\"Datetime\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# apply the transformation to the appropriate rows\n",
    "mask = (data_15min[\"Datetime\"] > bug_start) & (data_15min[\"Datetime\"] < bug_end)\n",
    "data_15min.loc[mask, \"Datetime\"] = data_15min.loc[mask, \"Datetime\"].apply(\n",
    "    lambda x: transform_date(x.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    ")\n",
    "print(\"-------- COMPLETE 2 -------\")\n",
    "\n",
    "\n",
    "print(\"-------- COMPLETE 5 -------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b874d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829fed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Save the data as a csv\n",
    "data_15min.to_csv(\"data/island_A/exported/data_15min_clean.csv\", index=False)\n",
    "\n",
    "data_15min_conv = data_15min\n",
    "print(\"-------- COMPLETE 6 -------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef32516",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_15min.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5318b367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Apply UTC Convention\n",
    "# View all timezones using the code below\n",
    "\"\"\"\n",
    ">>> import pytz\n",
    ">>> pytz.all_timezones\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def convert_to_utc(df, source_timezone):\n",
    "    # Ensure the DataFrame has a DatetimeIndex\n",
    "    #     if not isinstance(df.index, pd.DatetimeIndex):\n",
    "    #         raise ValueError(\"The DataFrame must have a DatetimeIndex.\")\n",
    "\n",
    "    # Create timezone objects for source and target (UTC) timezones\n",
    "    source_tz = pytz.timezone(source_timezone)\n",
    "    target_tz = pytz.UTC\n",
    "\n",
    "    # Convert the \"datetime\" column to a DatetimeIndex\n",
    "    # df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        # Convert the \"datetime\" column to a DatetimeIndex\n",
    "        df[\"Datetime\"] = pd.to_datetime(df[\"Datetime\"])\n",
    "        df.set_index(\"Datetime\", inplace=True)\n",
    "\n",
    "    # df.set_index('Datetime', inplace=True)\n",
    "\n",
    "    # Localize the DatetimeIndex to the source timezone, handling ambiguous and non-existent times\n",
    "    df_source_tz = df.index.tz_localize(source_tz, ambiguous=\"NaT\", nonexistent=\"NaT\")\n",
    "\n",
    "    # Convert the DatetimeIndex to the target timezone (UTC)\n",
    "    df_utc = df_source_tz.tz_convert(target_tz)\n",
    "\n",
    "    # Set the DatetimeIndex as a column in the DataFrame\n",
    "    df[\"datetimeUTC\"] = df_utc\n",
    "\n",
    "    df.set_index(\"datetimeUTC\", inplace=True)\n",
    "\n",
    "    # to get rid of the time zone different impliment:\n",
    "    # Format the DatetimeIndex without the timezone offset\n",
    "    # df_utc.index = df_utc.index.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# orginally got this error: NonExistentTimeError: 2019-03-31 02:00:00, so they dont adjust for daylight savings.\n",
    "\n",
    "\n",
    "source_timezone = \"Europe/Malta\"  # Replace with the desired timezone\n",
    "data_15min_utc = convert_to_utc(data_15min_conv, source_timezone)\n",
    "\n",
    "\n",
    "# will need to createa a lamda/function to go through all of the 15min data to convert it\n",
    "# to the correct UTC. It would be worth making this into a function so it can be used by the hourly.\n",
    "# Want it so that i just have to pass a dataframe that has a Datetime funciton into it.\n",
    "\n",
    "print(\"-------- COMPLETE 3 -------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339fec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_15min_utc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6250b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unused_15min(df):\n",
    "\n",
    "    df = df.drop(\n",
    "        [\"Time\", \"Year\", \"Month\", \"Day\", \"Time2\", \"Date\", \"Hour number\", \"Hourly Average\"], 1\n",
    "    )\n",
    "\n",
    "    df = df.reset_index()\n",
    "    df[\"datetimeUTC\"] = pd.to_datetime(df[\"datetimeUTC\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "data_15min_utc_drop = drop_unused_15min(data_15min_utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53561783",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_15min_utc_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1127fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Convert into Xarray and then NetCDF format\n",
    "# Converting to an Xarray\n",
    "def pdtocdf(df, file_name):\n",
    "\n",
    "    data_array = xr.Dataset(df)\n",
    "\n",
    "    data_array = data_array.set_coords(\"datetimeUTC\").swap_dims({\"dim_0\": \"datetimeUTC\"})\n",
    "\n",
    "    data_array = data_array.drop(\"dim_0\")\n",
    "\n",
    "    save_directory = \"./data/island_A/exported/\"\n",
    "    file_end = \".nc\"\n",
    "    full_file_path = save_directory + file_name + file_end\n",
    "\n",
    "    # Save the DataArray as a NetCDF file\n",
    "    data_array.to_netcdf(full_file_path)\n",
    "\n",
    "    print(f\"File saved at: {full_file_path}\")\n",
    "\n",
    "    return data_array\n",
    "\n",
    "\n",
    "data_15min_xarray = pdtocdf(data_15min_utc_drop, \"data_15min_FINv2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439b8fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_15min_xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39baa52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Create a new rolling sum of the 15min data\n",
    "data_15min_resample = data_15min\n",
    "\n",
    "# data_15min_rolling['15-Minute Output MWh'].rolling(4).sum()\n",
    "# data_15min_rolling['Power at point in time MW'].rolling(4).sum()\n",
    "\n",
    "\n",
    "# Assuming your DataFrame has a 'timestamp' column and columns 'A' and 'B'\n",
    "data_15min_resample[\"Datetime\"] = pd.to_datetime(data_15min_resample[\"Datetime\"])\n",
    "data_15min_resample.set_index(\"Datetime\", inplace=True)\n",
    "\n",
    "# Resample column 'B' to hourly frequency and sum the values\n",
    "df_B_hourly_sum_MWh = data_15min_resample[\"15-Minute Output MWh\"].resample(\"H\").sum()\n",
    "df_B_hourly_sum_MW = data_15min_resample[\"Power at point in time MW\"].resample(\"H\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1579e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Save the data as a csv\n",
    "df_B_hourly_sum_MWh.to_csv(\"data/island_A/exported/15mintohour_MWh.csv\", index=True)\n",
    "\n",
    "# 7. Save the data as a csv\n",
    "df_B_hourly_sum_MW.to_csv(\"data/island_A/exported/15mintohour_MW.csv\", index=True)\n",
    "\n",
    "print(\"-------- COMPLETE 7 -------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcff0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.5 Complete the normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e97cdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Save the data as a csv\n",
    "data_15min_rolling.to_csv(\"data/island_A/exported/data_15min_rolling.csv\", index=False)\n",
    "print(\"-------- COMPLETE 8 -------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fe48d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7849e027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder_data_load_sorted_h(folder_path):\n",
    "    # This is a list of all of the filenames\n",
    "    files = os.listdir(folder_path)\n",
    "\n",
    "    # Read all Excel files into a list of dataframes\n",
    "    dataframes = []\n",
    "\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".xlsx\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            df = pd.read_excel(file_path, engine=\"openpyxl\")\n",
    "\n",
    "            # This returns a tuple\n",
    "            dataframes.append((reference_date, df))\n",
    "\n",
    "    dataframes.sort(key=lambda x: x[0])\n",
    "\n",
    "    sorted_dataframes = [df for _, df in dataframes]\n",
    "\n",
    "    return sorted_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c0c766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best to create a way to look at all of the data now rather then just one specific year.\n",
    "def load_data_from_all_years_h(parent_folder_path_hourly):\n",
    "    # Initialize an empty list to store the dataframes\n",
    "    all_dataframes = []\n",
    "\n",
    "    # Loop over each year's folder and call the folder_data_load_sorted function\n",
    "\n",
    "    # folder_path = os.path.join(parent_folder_path, ,str(year))\n",
    "    dataframes = folder_data_load_sorted_h(parent_folder_path_hourly)\n",
    "    all_dataframes.extend(dataframes)\n",
    "\n",
    "    return all_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca52169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Hourly data -------------\n",
    "def load_all_hourly(parent_folder_path):\n",
    "\n",
    "    # all_dataframes = load_data_from_all_years_h(parent_folder_path)\n",
    "    # all_combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "    # This is a list of all of the filenames\n",
    "    files = os.listdir(parent_folder_path)\n",
    "\n",
    "    # Read all Excel files into a list of dataframes\n",
    "    dataframes = []\n",
    "\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".xlsx\"):\n",
    "            file_path = os.path.join(parent_folder_path, filename)\n",
    "\n",
    "            df = pd.read_excel(file_path, engine=\"openpyxl\")\n",
    "\n",
    "            # This returns a tuple\n",
    "            dataframes.append(df)\n",
    "\n",
    "    # dataframes.sort(key=lambda x: x[0])\n",
    "\n",
    "    # sorted_dataframes = [df for _, df in dataframes]\n",
    "\n",
    "    all_combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    # Sort the DataFrame based on the 'Datetime' column\n",
    "    all_combined_df_sort = all_combined_df.sort_values(by=\"Date\")\n",
    "\n",
    "    return all_combined_df_sort\n",
    "\n",
    "    # Sort the DataFrame based on the 'Datetime' column\n",
    "    # all_combined_df_sort = all_combined_df.sort_values(by=\"Datetime\")\n",
    "\n",
    "    # return all_combined_df_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7775fe11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the data into a single list\n",
    "parent_folder_path_hourly = \"./data/island_A/clean_data/hourly-PV/\"\n",
    "hourly_data_raw = load_all_hourly(parent_folder_path_hourly)\n",
    "print(\"--------- COMPLETE 1 --------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c1192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_data_raw.iloc[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16faaab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# 1. Load the data - Single excel file \n",
    "folder_path = \"./data/island_A/Hourly-PV/format_test/HourlyPVgeneratedUnits_2018_test.xlsx\"\n",
    "# folder_path = './data/island_A/15-min-PV/2019/2019-01-03_PVMalta.xlsx'\n",
    "hourly_data_raw = read_excel_file(folder_path)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93ca119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Convert into usable format (Transpose of hours)\n",
    "def transpose_data(df):\n",
    "\n",
    "    # Convert column names to integers\n",
    "    hour_columns = [col for col in df.columns if str(col).isdigit()]\n",
    "\n",
    "    # melt the data\n",
    "    # XXX Need to retain other information, edit this\n",
    "    melted = df.melt(\n",
    "        id_vars=[\n",
    "            \"Date\",\n",
    "            \" Total Max Capacity of Read Meters/KW\",\n",
    "            \"Total Max Capacity\",\n",
    "            \"Number of Read Meters\",\n",
    "            \"Total Number of Meters\",\n",
    "        ],\n",
    "        value_vars=hour_columns,\n",
    "        var_name=\"Hour\",\n",
    "    )\n",
    "\n",
    "    melted = melted.dropna()\n",
    "\n",
    "    melted[\"Date\"] = pd.to_datetime(melted[\"Date\"])\n",
    "    melted[\"Hour\"] = pd.to_timedelta(melted[\"Hour\"], unit=\"h\")\n",
    "\n",
    "    melted[\"Datetime\"] = melted[\"Date\"] + melted[\"Hour\"]\n",
    "\n",
    "    # Sort the DataFrame based on the 'Datetime' column\n",
    "    melted_sorted = melted.sort_values(by=\"Datetime\")\n",
    "\n",
    "    melted_sorted.rename(columns={\"value\": \"Hourly PV Generated Units\"}, inplace=True)\n",
    "\n",
    "    return melted_sorted\n",
    "\n",
    "\n",
    "hourly_data = transpose_data(hourly_data_raw)\n",
    "\n",
    "print(\"--------- COMPLETE 2 --------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb51b6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34d5756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the file\n",
    "hourly_data.to_csv(\"data/island_A/exported/hourly_data_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1a1f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Apply UTC conversion\n",
    "source_timezone = \"Europe/Malta\"  # Replace with the desired timezone\n",
    "data_hourly_utc = convert_to_utc(hourly_data, source_timezone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f710e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hourly_utc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bf9761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Drop unused information\n",
    "def drop_unused_hourly(df):\n",
    "\n",
    "    df = df.drop([\"Date\", \"Hour\"], 1)\n",
    "\n",
    "    df = df.reset_index()\n",
    "    df[\"datetimeUTC\"] = pd.to_datetime(df[\"datetimeUTC\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "data_hourly_utc_drop = drop_unused_hourly(data_hourly_utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4ce47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hourly_utc_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ab232e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936579f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Convert into Xarray and then NetCDF format\n",
    "data_hourly_xarray = pdtocdf(data_hourly_utc_drop, \"data_hourly_FIN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e686d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hourly_xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22496765",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0231efc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fbf923",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%history -g\n",
    "%save -r backuplog 1-999999\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a758516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%history -g -f session.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16634334",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
