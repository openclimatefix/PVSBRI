{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8ddecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db63cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- Import Libs ---------\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import math\n",
    "import pytz\n",
    "\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "alt.renderers.enable(\"altair_viewer\")\n",
    "\n",
    "# Disbale the max row limit for altair datasets.\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a8259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory: we use an environment variable defined in the Makefile.\n",
    "CWD = os.environ.get(\"CWD\")\n",
    "if CWD:\n",
    "    os.chdir(CWD)\n",
    "\n",
    "print(CWD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d8c203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function does the same as above but orders the dataframe correct\n",
    "# I should also make the folder path a variable so that I am perform some quick stats analysis on it\n",
    "def folder_data_load_sorted(folder_path):\n",
    "    # This is a list of all of the filenames\n",
    "    files = os.listdir(folder_path)\n",
    "\n",
    "    # Read all Excel files into a list of dataframes\n",
    "    dataframes = []\n",
    "\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".xlsx\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            print(file_path)\n",
    "\n",
    "            # Extract the reference date from the filename\n",
    "            reference_date = filename.split(\"_\")[0]\n",
    "\n",
    "            df = pd.read_excel(file_path, engine=\"openpyxl\")\n",
    "\n",
    "            # create new columns in the data for date specific\n",
    "            df[\"Year\"] = reference_date.split(\"-\")[0]\n",
    "            df[\"Month\"] = reference_date.split(\"-\")[1]\n",
    "            df[\"Day\"] = reference_date.split(\"-\")[2]\n",
    "\n",
    "            # Convert the 'Time' column to datetime.time objects\n",
    "            df[\"Time2\"] = pd.to_datetime(df[\"Time\"], format=\"%H:%M:%S\").dt.time\n",
    "            df[\"Time2\"] = df[\"Time2\"].astype(str)\n",
    "\n",
    "            df[\"Date\"] = pd.to_datetime(df[\"Year\"] + df[\"Month\"] + df[\"Day\"], format=\"%Y-%m-%d\")\n",
    "\n",
    "            # Convert the 'Time' column to datetime objects using the extracted reference date\n",
    "            df[\"Datetime\"] = pd.to_datetime(\n",
    "                reference_date + \" \" + df[\"Time2\"], format=\"%Y-%m-%d %H:%M:%S\"\n",
    "            )\n",
    "\n",
    "            # This returns a tuple\n",
    "            dataframes.append((reference_date, df))\n",
    "\n",
    "    dataframes.sort(key=lambda x: x[0])\n",
    "\n",
    "    sorted_dataframes = [df for _, df in dataframes]\n",
    "\n",
    "    return sorted_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f5aedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best to create a way to look at all of the data now rather then just one specific year.\n",
    "def load_data_from_all_years(parent_folder_path):\n",
    "    # Initialize an empty list to store the dataframes\n",
    "    all_dataframes = []\n",
    "\n",
    "    # Loop over each year's folder and call the folder_data_load_sorted function\n",
    "    for year in range(2018, 2024):\n",
    "        folder_path = os.path.join(parent_folder_path, str(year))\n",
    "        dataframes = folder_data_load_sorted(folder_path)\n",
    "        all_dataframes.extend(dataframes)\n",
    "\n",
    "    return all_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d853af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to load a single excel file\n",
    "def read_excel_file(file_path):\n",
    "    df = pd.read_excel(file_path, engine=\"openpyxl\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcc3fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 15 min data ------------\n",
    "def load_all_15min(parent_folder_path):\n",
    "\n",
    "    all_dataframes = load_data_from_all_years(parent_folder_path)\n",
    "    all_combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "    # Sort the DataFrame based on the 'Datetime' column\n",
    "    all_combined_df_sort = all_combined_df.sort_values(by=\"Datetime\")\n",
    "\n",
    "    return all_combined_df_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c48f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function to transform the dates to fix the bug\n",
    "def transform_date(date_str):\n",
    "    date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    date_obj = datetime.strptime(date_str, date_format)\n",
    "    new_date = date_obj.replace(hour=date_obj.minute, minute=date_obj.second, second=0)\n",
    "    return new_date.strftime(date_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208e8243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- 15 min data opperation ----------\n",
    "# 1. Load the data into a single list\n",
    "parent_folder_path = \"./data/island_A/clean_data/15-min-PV/\"\n",
    "data_15min = load_all_15min(parent_folder_path)\n",
    "\n",
    "print(\"-------- COMPLETE 1 -------\")\n",
    "\n",
    "# This code here is if you want to just load a single year of 15 min data\n",
    "#     folder_path = \"./data/island_A/15-min-PV/2019/\"\n",
    "#     data_15min = folder_data_load_sorted(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fae72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Fix the bugs with datetime interp\n",
    "# define the bug_start and bug_end dates + May need to enter new dates and check format\n",
    "bug_start = datetime.strptime(\"2018-12-19 23:45:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "bug_end = datetime.strptime(\"2019-10-05 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "data_15min[\"Datetime\"] = pd.to_datetime(data_15min[\"Datetime\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# apply the transformation to the appropriate rows\n",
    "mask = (data_15min[\"Datetime\"] > bug_start) & (data_15min[\"Datetime\"] < bug_end)\n",
    "data_15min.loc[mask, \"Datetime\"] = data_15min.loc[mask, \"Datetime\"].apply(\n",
    "    lambda x: transform_date(x.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    ")\n",
    "print(\"-------- COMPLETE 2 -------\")\n",
    "\n",
    "\n",
    "# 3. Apply UTC Convention\n",
    "# View all timezones using the code below\n",
    "\"\"\"\n",
    ">>> import pytz\n",
    ">>> pytz.all_timezones\n",
    "\"\"\"\n",
    "# will need to createa a lamda/function to go through all of the 15min data to convert it\n",
    "# to the correct UTC. It would be worth making this into a function so it can be used by the hourly.\n",
    "# Want it so that i just have to pass a dataframe that has a Datetime funciton into it.\n",
    "# local = pytz.timezone(\"Europe/Malta\")\n",
    "# naive = datetime.strptime(\"2001-2-3 10:11:12\", \"%Y-%m-%d %H:%M:%S\")\n",
    "# local_dt = local.localize(naive, is_dst=None)\n",
    "# utc_dt = local_dt.astimezone(pytz.utc)\n",
    "\n",
    "print(\"-------- COMPLETE 3 -------\")\n",
    "# 4 Strip unused info\n",
    "\n",
    "\n",
    "print(\"-------- COMPLETE 4 -------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829fed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Save the data as a csv\n",
    "data_15min.to_csv(\"data/island_A/exported/data_15min_clean.csv\", index=False)\n",
    "print(\"-------- COMPLETE 5 -------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39baa52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Create a new rolling sum of the 15min data\n",
    "data_15min_resample = data_15min\n",
    "\n",
    "# data_15min_rolling['15-Minute Output MWh'].rolling(4).sum()\n",
    "# data_15min_rolling['Power at point in time MW'].rolling(4).sum()\n",
    "\n",
    "\n",
    "# Assuming your DataFrame has a 'timestamp' column and columns 'A' and 'B'\n",
    "data_15min_resample[\"Datetime\"] = pd.to_datetime(data_15min_resample[\"Datetime\"])\n",
    "data_15min_resample.set_index(\"Datetime\", inplace=True)\n",
    "\n",
    "# Resample column 'B' to hourly frequency and sum the values\n",
    "df_B_hourly_sum_MWh = data_15min_resample[\"15-Minute Output MWh\"].resample(\"H\").sum()\n",
    "df_B_hourly_sum_MW = data_15min_resample[\"Power at point in time MW\"].resample(\"H\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1579e53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Save the data as a csv\n",
    "df_B_hourly_sum_MWh.to_csv(\"data/island_A/exported/15mintohour_MWh.csv\", index=True)\n",
    "\n",
    "# 7. Save the data as a csv\n",
    "df_B_hourly_sum_MW.to_csv(\"data/island_A/exported/15mintohour_MW.csv\", index=True)\n",
    "\n",
    "print(\"-------- COMPLETE 5 -------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcff0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.5 Complete the normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e97cdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Save the data as a csv\n",
    "data_15min_rolling.to_csv(\"data/island_A/exported/data_15min_rolling.csv\", index=False)\n",
    "print(\"-------- COMPLETE 5 -------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fe48d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Convert into Xarray and then NetCDF format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca52169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16faaab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Hourly data -------------\n",
    "# 1. Load the data\n",
    "folder_path = \"./data/island_A/Hourly-PV/format_test/HourlyPVgeneratedUnits_2018_test.xlsx\"\n",
    "# folder_path = './data/island_A/15-min-PV/2019/2019-01-03_PVMalta.xlsx'\n",
    "hourly_data_raw = read_excel_file(folder_path)\n",
    "\n",
    "print(\"--------- COMPLETE 1 --------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93ca119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Convert into usable format (Transpose of hours)\n",
    "def transpose_data(df):\n",
    "\n",
    "    # Convert column names to integers\n",
    "    hour_columns = [col for col in df.columns if str(col).isdigit()]\n",
    "\n",
    "    # melt the data\n",
    "    # XXX Need to retain other information, edit this\n",
    "    melted = df.melt(\n",
    "        id_vars=[\n",
    "            \"Date\",\n",
    "            \" Total Max Capacity of Read Meters/KW\",\n",
    "            \"Total Max Capacity\",\n",
    "            \"Number of Read Meters\",\n",
    "            \"Total Number of Meters\",\n",
    "        ],\n",
    "        value_vars=hour_columns,\n",
    "        var_name=\"Hour\",\n",
    "    )\n",
    "\n",
    "    melted = melted.dropna()\n",
    "\n",
    "    melted[\"Date\"] = pd.to_datetime(melted[\"Date\"])\n",
    "    melted[\"Hour\"] = pd.to_timedelta(melted[\"Hour\"], unit=\"h\")\n",
    "\n",
    "    melted[\"Datetime\"] = melted[\"Date\"] + melted[\"Hour\"]\n",
    "\n",
    "    # Sort the DataFrame based on the 'Datetime' column\n",
    "    melted_sorted = melted.sort_values(by=\"Datetime\")\n",
    "\n",
    "    return melted_sorted\n",
    "\n",
    "\n",
    "hourly_data = transpose_data(hourly_data_raw)\n",
    "\n",
    "print(\"--------- COMPLETE 2 --------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb51b6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hourly_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34d5756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the file\n",
    "hourly_data.to_csv(\"data/island_A/exported/hourly_data_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936579f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Convert into Xarray and then NetCDF format"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
