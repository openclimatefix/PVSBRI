{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d1070d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1982366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libs\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe6e249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's always annoying to set the working directory: we use an environment variable defined in the Makefile.\n",
    "CWD = os.environ.get(\"CWD\")\n",
    "if CWD:\n",
    "    os.chdir(CWD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68244eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2fc6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open Zarr as an xarray\n",
    "\n",
    "# link to one drive:\n",
    "\n",
    "\n",
    "path = \"./data/island/island_nwp_5y\"\n",
    "ds = xr.open_zarr(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55666ae2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674869c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates and sort by time\n",
    "def drop_dup_sort_time(ds):\n",
    "    time_series = pd.Series(ds[\"time\"].values)\n",
    "    unique_time_series = time_series.drop_duplicates()\n",
    "    unique_time_indices = unique_time_series.index\n",
    "\n",
    "    # Update dataset to include only the unique time indices\n",
    "    ds = ds.isel(time=unique_time_indices)\n",
    "    ds = ds.sortby(\"time\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09051093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to remove some other coordinates from the xarray in\n",
    "def remove_cords(ds):\n",
    "\n",
    "    unused_cords = [\n",
    "        \"atmosphere\",\n",
    "        \"boundaryLayerCloudLayer\",\n",
    "        \"convectiveCloudLayer\",\n",
    "        \"highCloudLayer\",\n",
    "        \"isobaricInhPa\",\n",
    "        \"lowCloudLayer\",\n",
    "        \"middleCloudLayer\",\n",
    "        \"surface\",\n",
    "    ]\n",
    "\n",
    "    for i in unused_cords:\n",
    "        ds = ds.drop(i)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a103ffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_variable_dims(ds):\n",
    "    #     Method v1\n",
    "    #     working  but not correct dim type (not a bold coord)\n",
    "    #     variables = list(ds.data_vars)\n",
    "    #     data_arrays = [ds[v] for v in variables]\n",
    "    #     new_da = xr.concat(data_arrays, dim =\"variables\")\n",
    "    #     dataset = xr.Dataset(dict(value=new_da))\n",
    "\n",
    "    #     dataset = dataset.set_coords('variables')\n",
    "    #     ds_var = ds.expand_dims({'variables': data_arrays})\n",
    "    #     new_ds = dataset.set_coords('variables')\n",
    "\n",
    "    #   Method v2\n",
    "    #  Get the list of data variables\n",
    "    variables = list(ds.data_vars)\n",
    "\n",
    "    # Create a new xarray with the data variables concatenated along a new dimension called \"variables\"\n",
    "    new_ds = xr.concat([ds[var] for var in variables], dim=\"variables\")\n",
    "\n",
    "    # Add the \"variables\" coordinate with 15 points\n",
    "    new_ds = new_ds.assign_coords(variables=variables)\n",
    "\n",
    "    return new_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b07edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_chunking(ds):\n",
    "    # Print chunk sizes for all variables in the dataset\n",
    "    for varname, var in ds.variables.items():\n",
    "        print(varname, var.chunks)\n",
    "\n",
    "    # Hard format the chunks\n",
    "    # ds = ds.chunk({'time': 1277, \"latitude\": 9, \"longitude\":9, \"step\": 10})\n",
    "\n",
    "    # can also use unify chunks\n",
    "    # ds = ds.unify_chunks()\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7368ecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(ds, path):\n",
    "    # save the dataset as a zarr file\n",
    "    ds.to_zarr(path, mode=\"w\")\n",
    "    print(\"file saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d72b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9446e26e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the script\n",
    "ds = drop_dup_sort_time(ds)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e577b42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds = remove_cords(ds)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3075c864",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds = reformat_variable_dims(ds)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27563071",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds = sort_chunking(ds)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ced621",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb9aae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you try to save you will run into chunking problems\n",
    "path = \"./data/island/NWP_v1\"\n",
    "save_dataset(ds, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f34253f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec928415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bec7c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbda6623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below here is dirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb67fc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.chunksizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c76b34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds.to_zarr(\"/Users/zakwatts/Coding/OCF/pv-site-prediction/data/island/chunking/t5/sorted\", mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9a9878",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = list(ds.data_vars)\n",
    "data_arrays = [ds[v] for v in variables]\n",
    "ds1_expanded = ds.expand_dims({\"variables\": data_arrays})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7758ed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5256567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = list(ds.data_vars)\n",
    "data_arrays = [ds[v] for v in variables]\n",
    "new_da = xr.concat(data_arrays, dim=\"variables\")\n",
    "dataset = xr.Dataset(dict(value=new_da))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3913120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ff4655",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cord_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9cc850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rechunk the 'value' variable to a uniform chunk size\n",
    "# rechunk the 'time' dimension to a uniform chunk size of 1\n",
    "dataset_cord_clean = dataset_cord_clean.chunk(\n",
    "    {\"time\": 20, \"latitude\": 9, \"longitude\": 9, \"variables\": 15, \"step\": 10}\n",
    ")\n",
    "\n",
    "# save the dataset as a zarr file\n",
    "dataset_cord_clean.to_zarr(\n",
    "    \"/Users/zakwatts/Coding/OCF/pv-site-prediction/data/island/chunking/t4\", mode=\"w\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb36cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr(\"/Users/zakwatts/Coding/OCF/pv-site-prediction/data/island/chunking/t4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4569bc22",
   "metadata": {},
   "source": [
    "ds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
